{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a3d0941",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center; font-size:40px;\">Transformer from scratch using only NumPy</h1>\n",
    "\n",
    "<h2 style=\"text-align:center; font-size:30px;\">No deep learning frameworks (e.g., PyTorch, TensorFlow)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b76a01e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f431573c",
   "metadata": {},
   "source": [
    "<h3 style=\"font-size:20px;\">1. Imports</h3>\n",
    "\n",
    "Imported are the `NumPy` library and a custom `TransformerHelper` class ( file: **transformer_helper.ipynb** ), written to keep the `Transformer` class clean and modular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f06206e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformer_helper import TransformerHelper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3187ae85",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dafc9a8",
   "metadata": {},
   "source": [
    "<h3 style=\"font-size:20px;\">2. Encoder and Decoder Classes</h3>\n",
    "\n",
    "The `Encoder` and `Decoder` classes contain the trainable parameters used to compute output probabilities during inference. They also store intermediate results required for backpropagation during training. The trainable parameters are initialised with pseudorandom values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f124248b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder():\n",
    "    def __init__(self, len_input_vocab, input_vocab, seed=7287):\n",
    "        self.input_vocab = input_vocab\n",
    "        rng = np.random.default_rng(seed)\n",
    "\n",
    "        #######################################################################################\n",
    "        ############ ENCODER LAYER TRAINABLE PARAMETERS #######################################\n",
    "        #######################################################################################\n",
    "\n",
    "        self.embedding_matrix = rng.standard_normal((len_input_vocab, 4))\n",
    "\n",
    "        self.W_keys_self_att_h_1    = rng.standard_normal((4, 4))\n",
    "        self.W_queries_self_att_h_1 = rng.standard_normal((4, 4))\n",
    "        self.W_values_self_att_h_1  = rng.standard_normal((4, 4))\n",
    "\n",
    "        self.W_keys_self_att_h_2    = rng.standard_normal((4, 4))\n",
    "        self.W_queries_self_att_h_2 = rng.standard_normal((4, 4))\n",
    "        self.W_values_self_att_h_2  = rng.standard_normal((4, 4))\n",
    "\n",
    "        self.W_keys_self_att_h_3    = rng.standard_normal((4, 4))\n",
    "        self.W_queries_self_att_h_3 = rng.standard_normal((4, 4))\n",
    "        self.W_values_self_att_h_3  = rng.standard_normal((4, 4))\n",
    "\n",
    "        self.W_output_self_att = rng.standard_normal((12, 4))\n",
    "\n",
    "        self.gamma_norm_self_att = rng.standard_normal((1, 4))\n",
    "        self.beta_norm_self_att  = rng.standard_normal((1, 4))\n",
    "\n",
    "        self.W_ffn_layer_1 = rng.standard_normal((4, 4))\n",
    "        self.W_ffn_layer_2 = rng.standard_normal((4, 4))\n",
    "\n",
    "        self.gamma_norm_ffn = rng.standard_normal((1, 4))\n",
    "        self.beta_norm_ffn  = rng.standard_normal((1, 4))\n",
    "        \n",
    "        \n",
    "        #######################################################################################\n",
    "        ############ ENCODER LAYER INTERMEDIATE CALCULATIONS ##################################\n",
    "        #######################################################################################\n",
    "        \n",
    "        self.position_encoded = None\n",
    "        \n",
    "        self.queries_self_att_h_1 = None\n",
    "        self.keys_self_att_h_1 = None\n",
    "        self.values_self_att_h_1 = None\n",
    "        self.scores_self_att_h_1 = None\n",
    "        self.scaled_self_att_h_1 = None\n",
    "        self.softmax_self_att_h_1 = None\n",
    "        self.output_self_att_h_1 = None\n",
    "\n",
    "        self.queries_self_att_h_2 = None\n",
    "        self.keys_self_att_h_2 = None\n",
    "        self.values_self_att_h_2 = None\n",
    "        self.scores_self_att_h_2 = None\n",
    "        self.scaled_self_att_h_2 = None\n",
    "        self.softmax_self_att_h_2 = None\n",
    "        self.output_self_att_h_2 = None\n",
    "\n",
    "        self.queries_self_att_h_3 = None\n",
    "        self.keys_self_att_h_3 = None\n",
    "        self.values_self_att_h_3 = None\n",
    "        self.scores_self_att_h_3 = None\n",
    "        self.scaled_self_att_h_3 = None\n",
    "        self.softmax_self_att_h_3 = None\n",
    "        self.output_self_att_h_3 = None\n",
    "\n",
    "        self.self_att_output = None\n",
    "        \n",
    "        self.added_self_att = None\n",
    "        self.norm_self_att = None\n",
    "        \n",
    "        self.ffn_layer_1 = None\n",
    "        self.relu_ffn = None\n",
    "        self.ffn_layer_2 = None\n",
    "        \n",
    "        self.added_ffn = None\n",
    "        self.norm_ffn = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d96c85f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder():\n",
    "    def __init__(self, len_output_vocab, output_vocab, seed=9373):\n",
    "        self.output_vocab = output_vocab\n",
    "        rng = np.random.default_rng(seed)  \n",
    "\n",
    "        #######################################################################################\n",
    "        ########################## DECODER LAYER TRAINABLE PARAMETERS #########################\n",
    "        #######################################################################################\n",
    "\n",
    "        self.embedding_matrix = rng.standard_normal((len_output_vocab, 4))\n",
    "\n",
    "        self.W_keys_masked_att_h_1    = rng.standard_normal((4, 4))\n",
    "        self.W_queries_masked_att_h_1 = rng.standard_normal((4, 4))\n",
    "        self.W_values_masked_att_h_1  = rng.standard_normal((4, 4))\n",
    "\n",
    "        self.W_keys_masked_att_h_2    = rng.standard_normal((4, 4))\n",
    "        self.W_queries_masked_att_h_2 = rng.standard_normal((4, 4))\n",
    "        self.W_values_masked_att_h_2  = rng.standard_normal((4, 4))\n",
    "\n",
    "        self.W_keys_masked_att_h_3    = rng.standard_normal((4, 4))\n",
    "        self.W_queries_masked_att_h_3 = rng.standard_normal((4, 4))\n",
    "        self.W_values_masked_att_h_3  = rng.standard_normal((4, 4))\n",
    "\n",
    "        self.W_output_masked_att = rng.standard_normal((12, 4))\n",
    "\n",
    "        self.gamma_norm_masked_att = rng.standard_normal((1, 4))\n",
    "        self.beta_norm_masked_att  = rng.standard_normal((1, 4))\n",
    "\n",
    "        self.W_keys_cross_att_h_1    = rng.standard_normal((4, 4))\n",
    "        self.W_queries_cross_att_h_1 = rng.standard_normal((4, 4))\n",
    "        self.W_values_cross_att_h_1  = rng.standard_normal((4, 4))\n",
    "\n",
    "        self.W_keys_cross_att_h_2    = rng.standard_normal((4, 4))\n",
    "        self.W_queries_cross_att_h_2 = rng.standard_normal((4, 4))\n",
    "        self.W_values_cross_att_h_2  = rng.standard_normal((4, 4))\n",
    "\n",
    "        self.W_keys_cross_att_h_3    = rng.standard_normal((4, 4))\n",
    "        self.W_queries_cross_att_h_3 = rng.standard_normal((4, 4))\n",
    "        self.W_values_cross_att_h_3  = rng.standard_normal((4, 4))\n",
    "\n",
    "        self.W_output_cross_att = rng.standard_normal((12, 4))\n",
    "\n",
    "        self.gamma_norm_cross_att = rng.standard_normal((1, 4))\n",
    "        self.beta_norm_cross_att  = rng.standard_normal((1, 4))\n",
    "\n",
    "        self.W_ffn_layer_1 = rng.standard_normal((4, 4))\n",
    "        self.W_ffn_layer_2 = rng.standard_normal((4, 4))\n",
    "\n",
    "        self.gamma_norm_ffn = rng.standard_normal((1, 4))\n",
    "        self.beta_norm_ffn  = rng.standard_normal((1, 4))\n",
    "\n",
    "        \n",
    "        #######################################################################################\n",
    "        ########################## DECODER LAYER INTERMEDIATE CALCULATIONS ####################\n",
    "        #######################################################################################\n",
    "\n",
    "        self.position_encoded = None\n",
    "        \n",
    "        self.queries_masked_att_h_1 = None\n",
    "        self.keys_masked_att_h_1 = None\n",
    "        self.values_masked_att_h_1 = None\n",
    "        self.scores_masked_att_h_1 = None\n",
    "        self.masked_scores_masked_att_h_1 = None\n",
    "        self.scaled_masked_att_h_1 = None\n",
    "        self.softmax_masked_att_h_1 = None\n",
    "        self.output_masked_att_h_1 = None\n",
    "\n",
    "        self.queries_masked_att_h_2 = None\n",
    "        self.keys_masked_att_h_2 = None\n",
    "        self.values_masked_att_h_2 = None\n",
    "        self.scores_masked_att_h_2 = None\n",
    "        self.masked_scores_masked_att_h_2 = None\n",
    "        self.scaled_masked_att_h_2 = None\n",
    "        self.softmax_masked_att_h_2 = None\n",
    "        self.output_masked_att_h_2 = None\n",
    "\n",
    "        self.queries_masked_att_h_3 = None\n",
    "        self.keys_masked_att_h_3 = None\n",
    "        self.values_masked_att_h_3 = None\n",
    "        self.scores_masked_att_h_3 = None\n",
    "        self.masked_scores_masked_att_h_3 = None\n",
    "        self.scaled_masked_att_h_3 = None\n",
    "        self.softmax_masked_att_h_3 = None\n",
    "        self.output_masked_att_h_3 = None\n",
    "\n",
    "        self.masked_att_output = None\n",
    "        \n",
    "        self.added_masked_att = None\n",
    "        self.norm_masked_att = None\n",
    "\n",
    "        self.keys_cross_att_h_1 = None\n",
    "        self.values_cross_att_h_1 = None\n",
    "        self.queries_cross_att_h_1 = None\n",
    "        self.scores_cross_att_h_1 = None\n",
    "        self.scaled_cross_att_h_1 = None\n",
    "        self.softmax_cross_att_h_1 = None\n",
    "        self.output_cross_att_h_1 = None\n",
    "\n",
    "        self.keys_cross_att_h_2 = None\n",
    "        self.values_cross_att_h_2 = None\n",
    "        self.queries_cross_att_h_2 = None\n",
    "        self.scores_cross_att_h_2 = None\n",
    "        self.scaled_cross_att_h_2 = None\n",
    "        self.softmax_cross_att_h_2 = None\n",
    "        self.output_cross_att_h_2 = None\n",
    "\n",
    "        self.keys_cross_att_h_3 = None\n",
    "        self.values_cross_att_h_3 = None\n",
    "        self.queries_cross_att_h_3 = None\n",
    "        self.scores_cross_att_h_3 = None\n",
    "        self.scaled_cross_att_h_3 = None\n",
    "        self.softmax_cross_att_h_3 = None\n",
    "        self.output_cross_att_h_3 = None\n",
    "\n",
    "        self.cross_att_output = None\n",
    "        \n",
    "        self.added_cross_att = None\n",
    "        self.norm_cross_att = None\n",
    "        \n",
    "        self.ffn_layer_1 = None\n",
    "        self.relu_ffn = None\n",
    "        self.ffn_layer_2 = None\n",
    "        \n",
    "        self.added_ffn = None\n",
    "        self.norm_ffn = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3467599",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36ca2de",
   "metadata": {},
   "source": [
    "<h3 style=\"font-size:20px;\">3. Transformer Class</h3>\n",
    "\n",
    "The `Transformer` class includes both training and inference routines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37088cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer():\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Define input and output vocabularies (token-to-index mappings)\n",
    "        self.input_vocab = { 'hello': 0, 'my': 1, 'name': 2, 'is': 3, 'messi': 4, '<eos>': 5, '<start>': 6 }\n",
    "        self.output_vocab = { 'hola': 0, 'mi': 1, 'nombre': 2, 'es': 3, 'messi': 4, '<eos>': 5, '<start>': 6 }\n",
    "\n",
    "        # Instantiate encoder and decoder with their respective vocabularies\n",
    "        self.encoder_layer = Encoder(len(self.input_vocab), self.input_vocab)\n",
    "        self.decoder_layer = Decoder(len(self.output_vocab), self.output_vocab)\n",
    "\n",
    "    #######################################################################################\n",
    "    ########################## TRAINING LOOP ##############################################\n",
    "    #######################################################################################\n",
    "\n",
    "    def train(self, train_set, num_of_epochs, learning_rate):\n",
    "        train_loss = []\n",
    "\n",
    "        for epoch_idx in range(num_of_epochs):\n",
    "            gradients_sum = {}                     # Accumulate gradients over the batch\n",
    "            losses_sum = 0                         # Total loss for this epoch\n",
    "            num_of_target_tokens_in_batch = 0      # Total number of target tokens processed\n",
    "\n",
    "            for (source_sequence, target_sequence) in train_set:\n",
    "                num_of_target_tokens_in_batch += len(target_sequence)\n",
    "\n",
    "                # Loop over each target token in the sequence\n",
    "                for target_token_idx, target_token in enumerate(target_sequence.split()):\n",
    "                    # Compute loss and gradients for the current target token\n",
    "                    loss_for_token, gradients = TransformerHelper.compute_loss_and_gradient(\n",
    "                        self,\n",
    "                        source_sequence, \n",
    "                        target_sequence, \n",
    "                        target_token_idx,\n",
    "                        self.output_vocab[target_token]\n",
    "                    )\n",
    "\n",
    "                    losses_sum += loss_for_token\n",
    "\n",
    "                    # Accumulate dense gradients (for weights, biases, etc.)\n",
    "                    for key, grad in gradients.items():\n",
    "                        if key not in [\n",
    "                            'decoder_layer.loss_wrt_input_embeddings',\n",
    "                            'encoder_layer.loss_wrt_input_embeddings'\n",
    "                        ]:\n",
    "                            if key not in gradients_sum:\n",
    "                                gradients_sum[key] = grad.copy()\n",
    "                            else:\n",
    "                                gradients_sum[key] += grad\n",
    "\n",
    "                    # Handle sparse gradients for decoder embeddings\n",
    "                    # Recreate the decoder input (all tokens before the current one)\n",
    "                    target_tokens = target_sequence.split()\n",
    "                    decoder_input_tokens = ['<start>'] + target_tokens[:target_token_idx]\n",
    "                    decoder_input_grads = gradients['decoder_layer.loss_wrt_input_embeddings']\n",
    "\n",
    "                    for i, token in enumerate(decoder_input_tokens):\n",
    "                        token_id = self.output_vocab[token]\n",
    "                        grad_i = decoder_input_grads[i]\n",
    "\n",
    "                        if 'decoder_layer.embedding_matrix' not in gradients_sum:\n",
    "                            gradients_sum['decoder_layer.embedding_matrix'] = np.zeros_like(\n",
    "                                self.decoder_layer.embedding_matrix)\n",
    "\n",
    "                        gradients_sum['decoder_layer.embedding_matrix'][token_id] += grad_i\n",
    "\n",
    "                    # Handle sparse gradients for encoder embeddings\n",
    "                    source_tokens = source_sequence.split()\n",
    "                    encoder_input_grads = gradients['encoder_layer.loss_wrt_input_embeddings']\n",
    "\n",
    "                    for i, token in enumerate(source_tokens):\n",
    "                        token_id = self.input_vocab[token]\n",
    "                        grad_i = encoder_input_grads[i]\n",
    "\n",
    "                        if 'encoder_layer.embedding_matrix' not in gradients_sum:\n",
    "                            gradients_sum['encoder_layer.embedding_matrix'] = np.zeros_like(\n",
    "                                self.encoder_layer.embedding_matrix)\n",
    "\n",
    "                        gradients_sum['encoder_layer.embedding_matrix'][token_id] += grad_i\n",
    "\n",
    "            # Normalize accumulated gradients by total number of target tokens\n",
    "            for key in gradients_sum:\n",
    "                gradients_sum[key] /= num_of_target_tokens_in_batch\n",
    "\n",
    "            # Apply gradients to update each parameter (basic SGD)\n",
    "            for key, grad in gradients_sum.items():\n",
    "                layer_name, param_name = key.split('.')\n",
    "                layer = getattr(self, layer_name)                 # e.g., self.encoder_layer\n",
    "                param = getattr(layer, param_name)                # e.g., encoder_layer.W_ffn\n",
    "                setattr(layer, param_name, param - learning_rate * grad)\n",
    "\n",
    "            # Store average loss for this epoch\n",
    "            train_loss.append(losses_sum / num_of_target_tokens_in_batch)\n",
    "\n",
    "        return train_loss\n",
    "\n",
    "    #######################################################################################\n",
    "    ########################## SEQUENCE TRANSFORMATION (INFERENCE) ########################\n",
    "    #######################################################################################\n",
    "\n",
    "    def transform(self, source_sequence, max_len=10):\n",
    "        predicted_tokens = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            # Construct decoder input (always starts with <start>, then predicted tokens)\n",
    "            decoder_input = ' '.join(['<start>'] + predicted_tokens)\n",
    "\n",
    "            # Run forward pass to get output token probabilities\n",
    "            output_probabilities = TransformerHelper.full_forward_pass(\n",
    "                self,\n",
    "                source_sequence,\n",
    "                decoder_input,\n",
    "                store_intermediate_results=False\n",
    "            )\n",
    "\n",
    "            # Select the token with the highest probability at the final position\n",
    "            probs = output_probabilities[-1]                # Shape: (vocab_size,)\n",
    "            predicted_token_id = np.argmax(probs)\n",
    "\n",
    "            # Convert token ID back to token string\n",
    "            inv_vocab = {v: k for k, v in self.output_vocab.items()}\n",
    "            predicted_token = inv_vocab[predicted_token_id]\n",
    "\n",
    "            # Stop if <eos> is generated\n",
    "            if predicted_token == '<eos>':\n",
    "                break\n",
    "\n",
    "            # Append predicted token and repeat\n",
    "            predicted_tokens.append(predicted_token)\n",
    "\n",
    "        return ' '.join(predicted_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfedab9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f0d0c4",
   "metadata": {},
   "source": [
    "<h3 style=\"font-size:20px;\">4. Demonstrating Transformer training and inference</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "408c200f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INFERENCES USING THE UNTRAINED MODEL\n",
      "------------------------------------------------------------------\n",
      "Source sequence: \"hello my name\"\n",
      "Target sequence: \"hola mi nombre <eos>\"\n",
      "MODEL OUTPUT \n",
      "model.transform(\"hello my name\"): \"messi messi messi messi messi messi messi messi messi messi\"\n",
      "------------------------------------------------------------------\n",
      "Source sequence: \"hello messi\"\n",
      "Target sequence: \"hola messi <eos>\"\n",
      "MODEL OUTPUT \n",
      "model.transform(\"hello messi\"): \"messi messi messi messi messi messi messi messi messi messi\"\n",
      "------------------------------------------------------------------\n",
      "Source sequence: \"my name is messi\"\n",
      "Target sequence: \"mi nombre es messi <eos>\"\n",
      "MODEL OUTPUT \n",
      "model.transform(\"my name is messi\"): \"messi messi messi messi messi messi messi messi messi messi\"\n",
      "\n",
      "======================================================================\n",
      "TRAINING\n",
      "\n",
      "Epoch     1: Loss: 0.5661\n",
      "Epoch   500: Loss: 0.3929\n",
      "Epoch  1000: Loss: 0.3819\n",
      "Epoch  1500: Loss: 0.3294\n",
      "Epoch  2000: Loss: 0.3197\n",
      "Epoch  2500: Loss: 0.2786\n",
      "Epoch  3000: Loss: 0.2662\n",
      "Epoch  3500: Loss: 0.2145\n",
      "Epoch  4000: Loss: 0.2002\n",
      "Epoch  4500: Loss: 0.1257\n",
      "Epoch  5000: Loss: 0.0963\n",
      "Epoch  5500: Loss: 0.0713\n",
      "Epoch  6000: Loss: 0.0642\n",
      "Epoch  6500: Loss: 0.0446\n",
      "Epoch  7000: Loss: 0.0557\n",
      "Epoch  7500: Loss: 0.0323\n",
      "Epoch  8000: Loss: 0.0275\n",
      "======================================================================\n",
      "\n",
      "INFERENCES USING THE TRAINED MODEL\n",
      "------------------------------------------------------------------\n",
      "Source sequence: \"hello my name\"\n",
      "Target sequence: \"hola mi nombre <eos>\"\n",
      "MODEL OUTPUT \n",
      "model.transform(\"hello my name\"): \"hola mi nombre\"\n",
      "------------------------------------------------------------------\n",
      "Source sequence: \"hello messi\"\n",
      "Target sequence: \"hola messi <eos>\"\n",
      "MODEL OUTPUT \n",
      "model.transform(\"hello messi\"): \"hola messi\"\n",
      "------------------------------------------------------------------\n",
      "Source sequence: \"my name is messi\"\n",
      "Target sequence: \"mi nombre es messi <eos>\"\n",
      "MODEL OUTPUT \n",
      "model.transform(\"my name is messi\"): \"mi nombre es messi\"\n"
     ]
    }
   ],
   "source": [
    "train_data = [\n",
    "    (\"hello my name\", \"hola mi nombre <eos>\"),\n",
    "    (\"hello messi\", \"hola messi <eos>\"),\n",
    "    (\"my name is messi\", \"mi nombre es messi <eos>\")\n",
    "]\n",
    "\n",
    "model = Transformer()\n",
    "\n",
    "print(\"\\nINFERENCES USING THE UNTRAINED MODEL\")\n",
    " \n",
    "for source_seq, target_seq in train_data:\n",
    "    print(\"------------------------------------------------------------------\")\n",
    "    print(f\"Source sequence: \\\"{source_seq}\\\"\")\n",
    "    print(f\"Target sequence: \\\"{target_seq}\\\"\")\n",
    "    model_output = model.transform(source_seq)\n",
    "    print(f\"MODEL OUTPUT \\nmodel.transform(\\\"{source_seq}\\\"): \\\"{model_output}\\\"\")\n",
    "\n",
    "print(\"\\n======================================================================\")\n",
    "print(\"TRAINING\")\n",
    "\n",
    "losses = model.train(train_data, num_of_epochs=8000, learning_rate=0.01)\n",
    "\n",
    "\n",
    "print(f\"\\nEpoch {1:>5}: Loss: {losses[0]:.4f}\")\n",
    "for epoch,loss in enumerate(losses):\n",
    "    if (epoch+1) % 500 == 0:\n",
    "        print(f\"Epoch {epoch + 1:>5}: Loss: {loss:.4f}\")\n",
    "print(\"======================================================================\")\n",
    "\n",
    "print(\"\\nINFERENCES USING THE TRAINED MODEL\")\n",
    " \n",
    "for source_seq, target_seq in train_data:\n",
    "    print(\"------------------------------------------------------------------\")\n",
    "    print(f\"Source sequence: \\\"{source_seq}\\\"\")\n",
    "    print(f\"Target sequence: \\\"{target_seq}\\\"\")\n",
    "    model_output = model.transform(source_seq)\n",
    "    print(f\"MODEL OUTPUT \\nmodel.transform(\\\"{source_seq}\\\"): \\\"{model_output}\\\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
